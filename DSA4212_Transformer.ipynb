{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Mode\n",
    "'''\n",
    "1. sequence reversal: 'seq_rev'\n",
    "2. basic arithmetic: 'basic_arith'\n",
    "3. copying task: 'copy'\n",
    "4. sorting numbers: 'sort'\n",
    "5. character-level text generation: 'text_gen'\n",
    "'''\n",
    "task_mode = 'text_gen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Character-Level Text Generation Task\n",
    "with open('data/alice_1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chars = list(text)\n",
    "char_counts = Counter(chars)\n",
    "\n",
    "vocab = list(char_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab)}\n",
    "int_to_char = {i: char for char, i in char_to_int.items()}\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "samples = [chars[i:i+SEQUENCE_LENGTH+1] for i in range(len(chars)-SEQUENCE_LENGTH)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, samples, char_to_int):\n",
    "        self.samples = samples\n",
    "        self.char_to_int = char_to_int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_seq = torch.LongTensor([self.char_to_int[word] for word in sample[:-1]])\n",
    "        target_seq = torch.LongTensor([self.char_to_int[word] for word in sample[1:]])\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Sorting Task\n",
    "class SortingDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10000, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = self.generate_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        for _ in range(self.num_samples):\n",
    "            seq = torch.randint(1, self.num_range, (self.seq_len,))\n",
    "            sorted_seq = torch.sort(seq)[0]  # Sorted sequence as target\n",
    "            data.append((seq, sorted_seq))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Sequence Reversal Task\n",
    "class SequenceReversalDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10000, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = [torch.randint(1, num_range, (seq_len,)) for _ in range(num_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx]\n",
    "        target_seq = torch.flip(input_seq, dims=[0])  # Reversed sequence as target\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Copying Task\n",
    "class CopyingTaskDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10000, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = [torch.randint(1, num_range, (seq_len,)) for _ in range(num_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx]\n",
    "        target_seq = input_seq.clone()  # Create a target sequence identical to the input\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: [6, 1, 5, 10, 1, 5, 5] | Target Sequence: [0, 7, 7, 0]\n",
      "Input Sequence: [4, 5, 9, 10, 5, 9, 9] | Target Sequence: [1, 0, 5, 8]\n",
      "Input Sequence: [4, 4, 8, 10, 5, 2, 2] | Target Sequence: [0, 9, 7, 0]\n",
      "Input Sequence: [8, 3, 9, 10, 6, 8, 8] | Target Sequence: [1, 5, 2, 7]\n",
      "Input Sequence: [5, 2, 9, 10, 7, 7, 5] | Target Sequence: [1, 3, 0, 4]\n",
      "Input Sequence: [7, 6, 9, 10, 2, 3, 7] | Target Sequence: [1, 0, 0, 6]\n",
      "Input Sequence: [3, 3, 6, 10, 7, 9, 6] | Target Sequence: [1, 1, 3, 2]\n",
      "Input Sequence: [1, 2, 2, 10, 4, 2, 1] | Target Sequence: [0, 5, 4, 3]\n",
      "Input Sequence: [4, 6, 1, 10, 5, 5, 9] | Target Sequence: [1, 0, 2, 0]\n",
      "Input Sequence: [2, 7, 9, 10, 2, 2, 9] | Target Sequence: [0, 5, 0, 8]\n"
     ]
    }
   ],
   "source": [
    "# Dataset Preparation for Basic Arithmetic Task\n",
    "\n",
    "# Digits 0-9 and '+' as 10\n",
    "digit = {str(i): i for i in range(10)}\n",
    "digit.update({\"+\": 10})  # Adding operator\n",
    "\n",
    "class BasicArithmeticDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Generate two random numbers and an operator\n",
    "            first_number = torch.randint(1, num_range, (self.seq_len // 2,))\n",
    "            second_number = torch.randint(1, num_range, (self.seq_len // 2,))\n",
    "            operation = \"+\"  # Only addition for now\n",
    "            \n",
    "            # Combine into a single sequence (input)\n",
    "            operation_int = digit[operation]\n",
    "            input_sequence = torch.cat((first_number, torch.tensor([operation_int]), second_number))\n",
    "            self.data.append(input_sequence)\n",
    "\n",
    "            # Perform the arithmetic operation and store the result (target)\n",
    "            result = self.perform_operation(first_number, second_number, operation)\n",
    "            self.targets.append(result)\n",
    "\n",
    "    def perform_operation(self, first_number, second_number, operation):\n",
    "        if operation == \"+\":\n",
    "            # Convert tensors to integers for addition\n",
    "            num1 = int(\"\".join(map(str, first_number.tolist())))\n",
    "            num2 = int(\"\".join(map(str, second_number.tolist())))\n",
    "            result = num1 + num2\n",
    "        \n",
    "        # Convert result to a tensor of digits and ensure it is length 4\n",
    "        result_tensor = torch.tensor([int(d) for d in f\"{result:04d}\"], dtype=torch.long)\n",
    "        return result_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Test the BasicArithmeticDataset\n",
    "dataset = BasicArithmeticDataset(seq_len=6, num_samples=10, num_range=10)\n",
    "\n",
    "# Print out the samples and their corresponding targets\n",
    "for i in range(len(dataset)):\n",
    "    input_seq, target_seq = dataset[i]\n",
    "    print(f\"Input Sequence: {input_seq.tolist()} | Target Sequence: {target_seq.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets based on task mode\n",
    "if task_mode == 'text_gen':\n",
    "    dataset = TextDataset(samples, char_to_int)\n",
    "    output_size = vocab_size  # For text generation, output is vocab size\n",
    "\n",
    "seq_len = 6\n",
    "num_samples = 10000\n",
    "num_range = 10\n",
    "\n",
    "if task_mode == 'basic_arith':\n",
    "    seq_len = 7\n",
    "\n",
    "if task_mode == 'sort':\n",
    "    dataset = SortingDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1  # For sorting, we output a single number per element in sequence\n",
    "if task_mode == 'seq_rev':\n",
    "    dataset = SequenceReversalDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1\n",
    "if task_mode == 'copy':\n",
    "    dataset = CopyingTaskDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1\n",
    "if task_mode == 'basic_arith':\n",
    "    dataset = BasicArithmeticDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Split the indices for training and testing\n",
    "def split_dataset(dataset, test_size=0.2):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_dataset(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for both train and test sets\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, dropout_rate=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, seq_len, output_size, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        if task_mode == 'text_gen':\n",
    "            self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim, dropout_rate=dropout_rate)\n",
    "        if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy' or task_mode == 'basic_arith':\n",
    "            self.pos_encoder = PositionalEncoding(max_len=seq_len, d_model=embed_dim, dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        if task_mode == 'text_gen':\n",
    "            self.linear = nn.Linear(embed_dim, output_size)  # Adjust output size dynamically\n",
    "        \n",
    "        if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy' or task_mode == 'basic_arith':\n",
    "            self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        input_mask = generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        x = self.pos_encoder(emb)\n",
    "\n",
    "        if task_mode == 'text_gen':\n",
    "            x = self.decoder(x, memory=x, tgt_mask=input_mask, memory_mask=input_mask)\n",
    "        \n",
    "        if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy' or task_mode == 'basic_arith':\n",
    "            x = self.decoder(x, memory=x, tgt_mask=input_mask)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-09 23:05:06,021] A new study created in memory with name: no-name-01220dbe-a2f2-4760-8edb-306fdd1ce9e7\n",
      "[I 2024-11-09 23:11:21,657] Trial 0 finished with value: 2.697094615581816 and parameters: {'num_heads': 7, 'embed_dim': 140, 'num_layers': 4, 'dropout_rate': 0.35284172631519806, 'lr': 2.3275509349611997e-05}. Best is trial 0 with value: 2.697094615581816.\n",
      "[I 2024-11-09 23:16:00,686] Trial 1 finished with value: 1.0546626591049464 and parameters: {'num_heads': 5, 'embed_dim': 100, 'num_layers': 3, 'dropout_rate': 0.11720846036957876, 'lr': 0.00060319525584865}. Best is trial 1 with value: 1.0546626591049464.\n",
      "[I 2024-11-09 23:17:51,586] Trial 2 finished with value: 2.3939122246429982 and parameters: {'num_heads': 6, 'embed_dim': 120, 'num_layers': 1, 'dropout_rate': 0.46044873279476706, 'lr': 0.0008686973925502818}. Best is trial 1 with value: 1.0546626591049464.\n",
      "[I 2024-11-09 23:27:03,157] Trial 3 finished with value: 2.8566290003008548 and parameters: {'num_heads': 8, 'embed_dim': 128, 'num_layers': 4, 'dropout_rate': 0.24878761813113132, 'lr': 0.006166109406768502}. Best is trial 1 with value: 1.0546626591049464.\n",
      "[I 2024-11-09 23:28:30,621] Trial 4 finished with value: 2.306873226587751 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 1, 'dropout_rate': 0.29416356446539627, 'lr': 0.001133894333144815}. Best is trial 1 with value: 1.0546626591049464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Loss: 1.0546626591049464\n",
      "  Best hyperparameters: \n",
      "    num_heads: 5\n",
      "    embed_dim: 100\n",
      "    num_layers: 3\n",
      "    dropout_rate: 0.11720846036957876\n",
      "    lr: 0.00060319525584865\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna to optimize\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters to tune\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8)\n",
    "    embed_dim = trial.suggest_int(\"embed_dim\", num_heads * 8, num_heads * 32, step=num_heads * 4)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Ensure embed_dim is divisible by num_heads\n",
    "    if embed_dim % num_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Define the model with the suggested hyperparameters\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        seq_len=seq_len,\n",
    "        output_size=output_size,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop parameters\n",
    "    epochs = 5\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for input_seq, target_seq in train_dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "            \n",
    "            # Apply the appropriate task mode setup for loss calculation\n",
    "            if task_mode == 'text_gen':\n",
    "                target_seq = target_seq.contiguous().view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode in ['sort', 'seq_rev']:\n",
    "                target_seq = target_seq.unsqueeze(1)\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            if task_mode == 'copy':\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode == 'basic_arith':\n",
    "                outputs = outputs[:, :target_seq.size(1), :]\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                target_seq = target_seq.reshape(-1)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Return the average loss as the metric to minimize\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "# Optuna study setup\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)  # Number of trials to search for optimal hyperparameters\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Loss: {trial.value}\")\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Training Setup\n",
    "\n",
    "if task_mode == 'seq_rev':\n",
    "    vocab_size = num_range\n",
    "\n",
    "if task_mode == 'copy':\n",
    "    output_size = num_range\n",
    "\n",
    "if task_mode == 'basic_arith':\n",
    "    vocab_size = num_range + 2\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=best_params['embed_dim'], \n",
    "    num_layers=best_params['num_layers'], \n",
    "    num_heads=best_params['num_heads'], \n",
    "    seq_len=seq_len,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 loss: 2.5653\n",
      "Epoch 2/10 loss: 2.1800\n",
      "Epoch 3/10 loss: 1.8435\n",
      "Epoch 4/10 loss: 1.4550\n",
      "Epoch 5/10 loss: 1.0969\n",
      "Epoch 6/10 loss: 0.8252\n",
      "Epoch 7/10 loss: 0.6373\n",
      "Epoch 8/10 loss: 0.5203\n",
      "Epoch 9/10 loss: 0.4437\n",
      "Epoch 10/10 loss: 0.3901\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train(model, epochs, dataloader, criterion, task_mode):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "            \n",
    "            if task_mode == 'text_gen':\n",
    "                target_seq = target_seq.contiguous().view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode == 'sort' or task_mode == 'seq_rev':\n",
    "                target_seq = target_seq.unsqueeze(1)  # Add a sequence dimension for targets\n",
    "                target_seq = target_seq.view(-1)  # Flatten target for loss calculation\n",
    "                outputs = outputs.view(-1, vocab_size)  # Flatten output for loss calculation\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            if task_mode == 'copy':\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode == 'basic_arith':\n",
    "                outputs = outputs[:, :4, :]\n",
    "                outputs = outputs.reshape(-1, vocab_size)  # Flatten outputs to (batch_size * seq_len, vocab_size)\n",
    "                target_seq = target_seq.reshape(-1)  # Flatten target sequence\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.detach().cpu().numpy()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch}/{epochs} loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Start training\n",
    "epochs = 10\n",
    "train(model, epochs, train_dataloader, criterion, task_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for Text Generation Task\n",
    "def return_int_vector(text):\n",
    "    chars = list(text)\n",
    "    input_seq = torch.LongTensor([char_to_int[char] for char in chars[-SEQUENCE_LENGTH:]]).unsqueeze(0)\n",
    "    return input_seq\n",
    "\n",
    "def sample_next(predictions):\n",
    "    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu()\n",
    "    next_token = torch.argmax(probabilities)\n",
    "    return int(next_token.cpu())\n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    model.eval()\n",
    "    sample = sentence\n",
    "    for i in range(generate_length):\n",
    "        int_vector = return_int_vector(sample)\n",
    "        if len(int_vector) >= SEQUENCE_LENGTH:\n",
    "            int_vector = int_vector[:, -SEQUENCE_LENGTH:]\n",
    "        input_tensor = int_vector.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "        next_token = sample_next(predictions)\n",
    "        sample += int_to_char[next_token]\n",
    "    print(sample)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for Sorting Task\n",
    "def sort_numbers(input_seq):\n",
    "    model.eval()\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_seq)  # Get the raw predictions\n",
    "        sorted_seq = torch.argmax(predictions, dim=2).squeeze(0).long().cpu().numpy()\n",
    "\n",
    "    return sorted(sorted_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for Sequence Reversal Task\n",
    "def reverse_sequence(input_seq):\n",
    "    model.eval()  \n",
    "    # Convert input sequence to a LongTensor and add batch dimension\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Shape: (1, seq_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure the input shape is (batch_size, seq_len) for transformer models\n",
    "        # Forward pass through the model\n",
    "        predictions = model(input_seq)\n",
    "        \n",
    "        # Get the predicted indices\n",
    "        reversed_seq = torch.argmax(predictions, dim=2).squeeze(0).long().cpu().numpy()  # Squeeze to remove the batch dimension\n",
    "    return reversed_seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for Copying Task\n",
    "def copy_sequence(input_seq):\n",
    "    model.eval()  \n",
    "    # Convert input sequence to a LongTensor and add batch dimension\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Shape: (1, seq_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure the input shape is (batch_size, seq_len) for transformer models\n",
    "        # Forward pass through the model\n",
    "        predictions = model(input_seq)\n",
    "        \n",
    "        # Get the predicted indices\n",
    "        reversed_seq = torch.argmax(predictions, dim=2).squeeze(0).long().cpu().numpy()  # Squeeze to remove the batch dimension\n",
    "    return reversed_seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for Basic Arithmetic Task\n",
    "def basic_arithmetic(input_seq):\n",
    "    model.eval()\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_seq)  # Get the raw predictions\n",
    "        \n",
    "        # Restrict predictions to 4 time steps (matching expected target length)\n",
    "        predictions = predictions[:, :4, :]\n",
    "        result_seq = torch.argmax(predictions, dim=-1).squeeze(0).long().cpu().numpy()\n",
    "\n",
    "    return result_seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examples:\n",
    "if task_mode == 'text_gen':\n",
    "    initial_text = [\"Alice was\"]\n",
    "    generate_length = 100\n",
    "    for sentence in initial_text:\n",
    "        print(f\"PROMPT: {sentence}\")\n",
    "        text_generator(sentence, generate_length)\n",
    "\n",
    "elif task_mode == 'sort':\n",
    "    input_seq = [1, 3, 2, 4, 5, 3]\n",
    "    print(\"Original Sequence:\", input_seq)\n",
    "    sorted_seq = sort_numbers(input_seq)\n",
    "    print(\"Sorted Sequence:\", sorted_seq)\n",
    "    \n",
    "elif task_mode == 'seq_rev':\n",
    "    input_seq = [1, 3, 2, 4, 5, 3]\n",
    "    print(\"Original Sequence:\", input_seq)\n",
    "    reversed_seq = reverse_sequence(input_seq)\n",
    "    print(\"Reversed Sequence:\", reversed_seq)\n",
    "\n",
    "elif task_mode == 'copy':\n",
    "    input_seq = [1, 3, 2, 4, 5, 3]\n",
    "    print(\"Input Sequence:\", input_seq)\n",
    "    copy_seq = copy_sequence(input_seq)\n",
    "    print(\"Copied Sequence:\", copy_seq)\n",
    "\n",
    "elif task_mode == 'basic_arith':\n",
    "    input_seq = [1, 3, 2, digit[\"+\"], 9, 4, 2]\n",
    "    print(\"Input Sequence:\", input_seq)\n",
    "    arith_seq = basic_arithmetic(input_seq)\n",
    "    print(\"Arithmetic Sequence:\", arith_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1772, Test Accuracy: 0.9450\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Evaluation\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, criterion, task_mode):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "\n",
    "            # Reshape the outputs and targets for the respective tasks\n",
    "            if task_mode == 'text_gen':\n",
    "                target_seq = target_seq.contiguous().view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "            \n",
    "            if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy':\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "\n",
    "            if task_mode == 'basic_arith':\n",
    "                outputs = outputs[:, :target_seq.size(1), :]\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                target_seq = target_seq.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, target_seq)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target_seq.detach().cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "test_loss, test_accuracy = evaluate_model(model, test_dataloader, criterion, task_mode=task_mode)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulated Results:\n",
      "Character-Level Text Generation Task:\n",
      "Prompt [1]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [2]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [3]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [4]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [5]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [6]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [7]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [8]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [9]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n",
      "Prompt [10]: Alice was\n",
      "Alice was the Queen for help in returning home, a sudden commotion erupted in the palace courtyard. The Knave\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate inputs for evaluation\n",
    "def simulate_inputs(task_mode, num_samples=10):\n",
    "    simulated_inputs = []\n",
    "    for _ in range(num_samples):\n",
    "        if task_mode == 'basic_arith':\n",
    "            first_number = torch.randint(1, num_range, (seq_len // 2,)).tolist()\n",
    "            second_number = torch.randint(1, num_range, (seq_len // 2,)).tolist()\n",
    "            input_seq = first_number + [digit[\"+\"]] + second_number\n",
    "            simulated_inputs.append(input_seq)\n",
    "        elif task_mode == 'seq_rev':\n",
    "            input_seq = torch.randint(1, num_range, (seq_len,)).tolist()\n",
    "            simulated_inputs.append(input_seq)\n",
    "        elif task_mode == 'copy':\n",
    "            input_seq = torch.randint(1, num_range, (seq_len,)).tolist()\n",
    "            simulated_inputs.append(input_seq)\n",
    "        elif task_mode == 'sort':\n",
    "            input_seq = torch.randint(1, num_range, (seq_len,)).tolist()\n",
    "            simulated_inputs.append(input_seq)\n",
    "        elif task_mode == 'text_gen':\n",
    "            sample_text = \"Alice was\"\n",
    "            simulated_inputs.append(sample_text)\n",
    "    return simulated_inputs\n",
    "\n",
    "# Run simulated inputs and check the results\n",
    "simulated_inputs = simulate_inputs(task_mode, num_samples=10)\n",
    "\n",
    "print(\"\\nSimulated Results:\")\n",
    "\n",
    "if task_mode == 'basic_arith':\n",
    "    print(\"Basic Arithmetic Task:\")\n",
    "    for idx, input_seq in enumerate(simulated_inputs):\n",
    "        arith_seq = basic_arithmetic(input_seq)\n",
    "        print(f\"Input Sequence [{idx+1}]: {input_seq}\")\n",
    "        print(f\"Arithmetic Result [{idx+1}]: {arith_seq}\\n\")\n",
    "\n",
    "elif task_mode == 'seq_rev':\n",
    "    print(\"Sequence Reversal Task:\")\n",
    "    for idx, input_seq in enumerate(simulated_inputs):\n",
    "        reversed_seq = reverse_sequence(input_seq)\n",
    "        print(f\"Input Sequence [{idx+1}]: {input_seq}\")\n",
    "        print(f\"Reversed Sequence [{idx+1}]: {reversed_seq}\\n\")\n",
    "\n",
    "elif task_mode == 'copy':\n",
    "    print(\"Copying Task:\")\n",
    "    for idx, input_seq in enumerate(simulated_inputs):\n",
    "        copied_seq = copy_sequence(input_seq)\n",
    "        print(f\"Input Sequence [{idx+1}]: {input_seq}\")\n",
    "        print(f\"Copied Sequence [{idx+1}]: {copied_seq}\\n\")\n",
    "\n",
    "elif task_mode == 'sort':\n",
    "    print(\"Sorting Task:\")\n",
    "    for idx, input_seq in enumerate(simulated_inputs):\n",
    "        sorted_seq = sort_numbers(input_seq)\n",
    "        print(f\"Input Sequence [{idx+1}]: {input_seq}\")\n",
    "        print(f\"Sorted Sequence [{idx+1}]: {sorted_seq}\\n\")\n",
    "\n",
    "elif task_mode == 'text_gen':\n",
    "    print(\"Character-Level Text Generation Task:\")\n",
    "    generate_length = 100\n",
    "    for idx, sentence in enumerate(simulated_inputs):\n",
    "        print(f\"Prompt [{idx+1}]: {sentence}\")\n",
    "        text_generator(sentence, generate_length)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
