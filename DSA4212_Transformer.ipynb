{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Mode\n",
    "'''\n",
    "1. sequence reversal: 'seq_rev'\n",
    "2. basic arithmetic: 'basic_arith'\n",
    "3. copying task: 'copy'\n",
    "4. sorting numbers: 'sort'\n",
    "5. character-level text generation: 'text_gen'\n",
    "'''\n",
    "task_mode = 'text_gen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset preparation for character-level text generation\n",
    "with open('data/alice_1.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chars = list(text)\n",
    "char_counts = Counter(chars)\n",
    "\n",
    "vocab = list(char_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "char_to_int = {char: i for i, char in enumerate(vocab)}\n",
    "int_to_char = {i: char for char, i in char_to_int.items()}\n",
    "\n",
    "SEQUENCE_LENGTH = 64\n",
    "samples = [chars[i:i+SEQUENCE_LENGTH+1] for i in range(len(chars)-SEQUENCE_LENGTH)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, samples, char_to_int):\n",
    "        self.samples = samples\n",
    "        self.char_to_int = char_to_int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        input_seq = torch.LongTensor([self.char_to_int[word] for word in sample[:-1]])\n",
    "        target_seq = torch.LongTensor([self.char_to_int[word] for word in sample[1:]])\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Sorting Task\n",
    "class SortingDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10000, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = self.generate_data()\n",
    "\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        for _ in range(self.num_samples):\n",
    "            seq = torch.randint(1, self.num_range, (self.seq_len,))\n",
    "            sorted_seq = torch.sort(seq)[0]  # Sorted sequence as target\n",
    "            data.append((seq, sorted_seq))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Sequence Reversal Task\n",
    "class SequenceReversalDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10000, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = [torch.randint(1, num_range, (seq_len,)) for _ in range(num_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx]\n",
    "        target_seq = torch.flip(input_seq, dims=[0])  # Reversed sequence as target\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation for Copying Task\n",
    "class CopyingTaskDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10000, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = [torch.randint(1, num_range, (seq_len,)) for _ in range(num_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx]\n",
    "        target_seq = input_seq.clone()  # Create a target sequence identical to the input\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: [5, 4, 2, 10, 3, 1, 7] | Target Sequence: [0, 8, 5, 9]\n",
      "Input Sequence: [2, 7, 7, 10, 7, 7, 4] | Target Sequence: [1, 0, 5, 1]\n",
      "Input Sequence: [7, 1, 1, 10, 6, 6, 6] | Target Sequence: [1, 3, 7, 7]\n",
      "Input Sequence: [2, 5, 7, 10, 8, 7, 1] | Target Sequence: [1, 1, 2, 8]\n",
      "Input Sequence: [8, 1, 9, 10, 4, 6, 3] | Target Sequence: [1, 2, 8, 2]\n",
      "Input Sequence: [7, 1, 7, 10, 9, 6, 7] | Target Sequence: [1, 6, 8, 4]\n",
      "Input Sequence: [1, 4, 2, 10, 1, 2, 4] | Target Sequence: [0, 2, 6, 6]\n",
      "Input Sequence: [4, 8, 9, 10, 3, 7, 6] | Target Sequence: [0, 8, 6, 5]\n",
      "Input Sequence: [9, 1, 9, 10, 3, 1, 7] | Target Sequence: [1, 2, 3, 6]\n",
      "Input Sequence: [5, 3, 2, 10, 8, 7, 9] | Target Sequence: [1, 4, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Digits 0-9 and '+' as 10\n",
    "digit = {str(i): i for i in range(10)}\n",
    "digit.update({\"+\": 10})  # Adding operator\n",
    "\n",
    "class BasicArithmeticDataset(Dataset):\n",
    "    def __init__(self, seq_len=6, num_samples=10, num_range=10):\n",
    "        self.seq_len = seq_len\n",
    "        self.num_samples = num_samples\n",
    "        self.num_range = num_range\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Generate two random numbers and an operator\n",
    "            first_number = torch.randint(1, num_range, (self.seq_len // 2,))\n",
    "            second_number = torch.randint(1, num_range, (self.seq_len // 2,))\n",
    "            operation = \"+\"  # Can be extended for other operations\n",
    "            \n",
    "            # Convert the operator to its corresponding integer (10)\n",
    "            operation_int = digit[operation]\n",
    "\n",
    "            # Combine into a single sequence (input)\n",
    "            input_sequence = torch.cat((first_number, torch.tensor([operation_int]), second_number))\n",
    "            self.data.append(input_sequence)\n",
    "\n",
    "            # Perform the arithmetic operation and store the result (target)\n",
    "            result = self.perform_operation(first_number, second_number, operation)\n",
    "            self.targets.append(result)\n",
    "\n",
    "    def perform_operation(self, first_number, second_number, operation):\n",
    "        if operation == \"+\":\n",
    "            # Convert the tensors to integers for addition\n",
    "            num1 = int(\"\".join(map(str, first_number.tolist())))\n",
    "            num2 = int(\"\".join(map(str, second_number.tolist())))\n",
    "            result = num1 + num2\n",
    "        \n",
    "        # Convert result to a tensor of digits\n",
    "        # result_tensor = torch.tensor([int(d) for d in str(result)])\n",
    "        result_tensor = torch.tensor([int(d) for d in str(result)], dtype=torch.long)\n",
    "\n",
    "        # Prepend 0 if the result has only 3 digits\n",
    "        if result_tensor.size(0) < 4:  # Adjusted to check if < 4 digits\n",
    "            result_tensor = torch.cat((torch.tensor([0]), result_tensor))\n",
    "        \n",
    "        return result_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.data[idx]\n",
    "        target_seq = self.targets[idx]\n",
    "        return input_seq, target_seq\n",
    "\n",
    "# Test the BasicArithmeticDataset\n",
    "dataset = BasicArithmeticDataset(seq_len=6, num_samples=10, num_range=10)\n",
    "\n",
    "# Print out the samples and their corresponding targets\n",
    "for i in range(len(dataset)):\n",
    "    input_seq, target_seq = dataset[i]\n",
    "    print(f\"Input Sequence: {input_seq.tolist()} | Target Sequence: {target_seq.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets based on task mode\n",
    "if task_mode == 'text_gen':\n",
    "    dataset = TextDataset(samples, char_to_int)\n",
    "    output_size = vocab_size  # For text generation, output is vocab size\n",
    "\n",
    "seq_len = 6\n",
    "num_samples = 10000\n",
    "num_range = 10\n",
    "\n",
    "if task_mode == 'basic_arith':\n",
    "    seq_len = 7\n",
    "\n",
    "if task_mode == 'sort':\n",
    "    dataset = SortingDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1  # For sorting, we output a single number per element in sequence\n",
    "if task_mode == 'seq_rev':\n",
    "    dataset = SequenceReversalDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1\n",
    "if task_mode == 'copy':\n",
    "    dataset = CopyingTaskDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1\n",
    "if task_mode == 'basic_arith':\n",
    "    dataset = BasicArithmeticDataset(seq_len=seq_len, num_samples=num_samples, num_range=num_range)\n",
    "    output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Split the indices for training and testing\n",
    "def split_dataset(dataset, test_size=0.2):\n",
    "    dataset_size = len(dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = split_dataset(dataset, test_size=0.2)\n",
    "#len(train_dataset)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for both train and test sets\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding Class\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, dropout_rate=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_layers, num_heads, seq_len, output_size, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        if task_mode == 'text_gen':\n",
    "            self.pos_encoder = PositionalEncoding(max_len=SEQUENCE_LENGTH, d_model=embed_dim, dropout_rate=dropout_rate)\n",
    "        if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy' or task_mode == 'basic_arith':\n",
    "            self.pos_encoder = PositionalEncoding(max_len=seq_len, d_model=embed_dim, dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=self.decoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        if task_mode == 'text_gen':\n",
    "            self.linear = nn.Linear(embed_dim, output_size)  # Adjust output size dynamically\n",
    "        \n",
    "        if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy' or task_mode == 'basic_arith':\n",
    "            self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb = self.emb(x)\n",
    "        input_mask = generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        x = self.pos_encoder(emb)\n",
    "\n",
    "        if task_mode == 'text_gen':\n",
    "            x = self.decoder(x, memory=x, tgt_mask=input_mask, memory_mask=input_mask)\n",
    "        \n",
    "        if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy' or task_mode == 'basic_arith':\n",
    "            x = self.decoder(x, memory=x, tgt_mask=input_mask)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        out = self.linear(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Training Setup\n",
    "\n",
    "if task_mode == 'seq_rev':\n",
    "    vocab_size = num_range\n",
    "\n",
    "if task_mode == 'copy':\n",
    "    output_size = num_range\n",
    "\n",
    "if task_mode == 'basic_arith':\n",
    "    vocab_size = num_range + 2\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size, \n",
    "    embed_dim=100, \n",
    "    num_layers=2, \n",
    "    num_heads=2, \n",
    "    seq_len=seq_len,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=0.2\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 loss: 2.5730\n",
      "Epoch 2/10 loss: 2.2453\n",
      "Epoch 3/10 loss: 2.0381\n",
      "Epoch 4/10 loss: 1.8205\n",
      "Epoch 5/10 loss: 1.6035\n",
      "Epoch 6/10 loss: 1.3993\n",
      "Epoch 7/10 loss: 1.2110\n",
      "Epoch 8/10 loss: 1.0483\n",
      "Epoch 9/10 loss: 0.9198\n",
      "Epoch 10/10 loss: 0.8149\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train(model, epochs, dataloader, criterion, task_mode):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        running_loss = 0\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "            \n",
    "            if task_mode == 'text_gen':\n",
    "                target_seq = target_seq.contiguous().view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode == 'sort' or task_mode == 'seq_rev':\n",
    "                target_seq = target_seq.unsqueeze(1)  # Add a sequence dimension for targets\n",
    "                target_seq = target_seq.view(-1)  # Flatten target for loss calculation\n",
    "                outputs = outputs.view(-1, vocab_size)  # Flatten output for loss calculation\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            if task_mode == 'copy':\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode == 'basic_arith':\n",
    "                outputs = outputs[:, :target_seq.size(1), :]\n",
    "                outputs = outputs.reshape(-1, vocab_size)  # Flatten outputs to (batch_size * seq_len, vocab_size)\n",
    "                target_seq = target_seq.reshape(-1)  # Flatten target sequence\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.detach().cpu().numpy()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch}/{epochs} loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Start training\n",
    "epochs = 10\n",
    "train(model, epochs, train_dataloader, criterion, task_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: For Text Generation Task\n",
    "def return_int_vector(text):\n",
    "    chars = list(text)\n",
    "    input_seq = torch.LongTensor([char_to_int[char] for char in chars[-SEQUENCE_LENGTH:]]).unsqueeze(0)\n",
    "    return input_seq\n",
    "\n",
    "def sample_next(predictions):\n",
    "    probabilities = F.softmax(predictions[:, -1, :], dim=-1).cpu()\n",
    "    next_token = torch.argmax(probabilities)\n",
    "    return int(next_token.cpu())\n",
    "\n",
    "def text_generator(sentence, generate_length):\n",
    "    model.eval()\n",
    "    sample = sentence\n",
    "    for i in range(generate_length):\n",
    "        int_vector = return_int_vector(sample)\n",
    "        if len(int_vector) >= SEQUENCE_LENGTH:\n",
    "            int_vector = int_vector[:, -SEQUENCE_LENGTH:]\n",
    "        input_tensor = int_vector.to(device)\n",
    "        with torch.no_grad():\n",
    "            predictions = model(input_tensor)\n",
    "        next_token = sample_next(predictions)\n",
    "        sample += int_to_char[next_token]\n",
    "    print(sample)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for Sorting Task\n",
    "def sort_numbers(input_seq):\n",
    "    model.eval()\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_seq)  # Get the raw predictions\n",
    "        sorted_seq = torch.argmax(predictions, dim=2).squeeze(0).long().cpu().numpy()\n",
    "\n",
    "    return sorted(sorted_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for sequence reversal task\n",
    "def reverse_sequence(input_seq):\n",
    "    model.eval()  \n",
    "    # Convert input sequence to a LongTensor and add batch dimension\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Shape: (1, seq_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure the input shape is (batch_size, seq_len) for transformer models\n",
    "        # Forward pass through the model\n",
    "        predictions = model(input_seq)\n",
    "        \n",
    "        # Get the predicted indices\n",
    "        reversed_seq = torch.argmax(predictions, dim=2).squeeze(0).long().cpu().numpy()  # Squeeze to remove the batch dimension\n",
    "    return reversed_seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for copying task\n",
    "def copy_sequence(input_seq):\n",
    "    model.eval()  \n",
    "    # Convert input sequence to a LongTensor and add batch dimension\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Shape: (1, seq_length)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ensure the input shape is (batch_size, seq_len) for transformer models\n",
    "        # Forward pass through the model\n",
    "        predictions = model(input_seq)\n",
    "        \n",
    "        # Get the predicted indices\n",
    "        reversed_seq = torch.argmax(predictions, dim=2).squeeze(0).long().cpu().numpy()  # Squeeze to remove the batch dimension\n",
    "    return reversed_seq.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for basic arithmetic task\n",
    "def basic_arithmetic(input_seq):\n",
    "    model.eval()\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_seq)  # Get the raw predictions\n",
    "        result_seq = torch.argmax(predictions, dim=-1).squeeze(0).long().cpu().numpy()\n",
    "\n",
    "        result_seq = result_seq[result_seq != 0] \n",
    "        return result_seq.tolist()  # Just return as is, without hardcoded length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Alice was\n",
      "Alice was small dooor that seemed to lead to a garden. However, to her surprise, she couldn't fit through the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "if task_mode == 'text_gen':\n",
    "    initial_text = [\"Alice was\"]\n",
    "    generate_length = 100\n",
    "    for sentence in initial_text:\n",
    "        print(f\"PROMPT: {sentence}\")\n",
    "        text_generator(sentence, generate_length)\n",
    "\n",
    "elif task_mode == 'sort':\n",
    "    input_seq = [1, 3, 2, 4, 5, 3]\n",
    "    print(\"Original Sequence:\", input_seq)\n",
    "    sorted_seq = sort_numbers(input_seq)\n",
    "    print(\"Sorted Sequence:\", sorted_seq)\n",
    "    \n",
    "elif task_mode == 'seq_rev':\n",
    "    input_seq = [1, 3, 2, 4, 5, 3]\n",
    "    print(\"Original Sequence:\", input_seq)\n",
    "    reversed_seq = reverse_sequence(input_seq)\n",
    "    print(\"Reversed Sequence:\", reversed_seq)\n",
    "\n",
    "elif task_mode == 'copy':\n",
    "    input_seq = [1, 3, 2, 4, 5, 3]\n",
    "    print(\"Input Sequence:\", input_seq)\n",
    "    copy_seq = copy_sequence(input_seq)\n",
    "    print(\"Copied Sequence:\", copy_seq)\n",
    "\n",
    "elif task_mode == 'basic_arith':\n",
    "    input_seq = [1, 3, 2, digit[\"+\"], 9, 4, 2]\n",
    "    print(\"Input Sequence:\", input_seq)\n",
    "    arith_seq = basic_arithmetic(input_seq)\n",
    "    print(\"Arithmetic Sequence:\", arith_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2421, Test Accuracy: 0.9277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader, criterion, task_mode):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "\n",
    "            # Reshape the outputs and targets for the respective tasks\n",
    "            if task_mode == 'text_gen':\n",
    "                target_seq = target_seq.contiguous().view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "            \n",
    "            if task_mode == 'sort' or task_mode == 'seq_rev' or task_mode == 'copy':\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "\n",
    "            if task_mode == 'basic_arith':\n",
    "                outputs = outputs[:, :target_seq.size(1), :]\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                target_seq = target_seq.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, target_seq)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions\n",
    "            preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(target_seq.detach().cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "test_loss, test_accuracy = evaluate_model(model, test_dataloader, criterion, task_mode=task_mode)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-31 14:39:30,222] A new study created in memory with name: no-name-a0e949d2-faff-457a-94b1-72fa405d91a4\n",
      "[I 2024-10-31 14:42:26,661] Trial 0 finished with value: 2.511040056701255 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 3, 'dropout_rate': 0.3482229230519521, 'lr': 0.00022345065100274396}. Best is trial 0 with value: 2.511040056701255.\n",
      "[I 2024-10-31 14:46:41,954] Trial 1 finished with value: 2.209451403238077 and parameters: {'num_heads': 6, 'embed_dim': 144, 'num_layers': 3, 'dropout_rate': 0.40393297441247833, 'lr': 0.0021520829504558655}. Best is trial 1 with value: 2.209451403238077.\n",
      "[I 2024-10-31 14:49:56,629] Trial 2 finished with value: 2.6115314390807027 and parameters: {'num_heads': 2, 'embed_dim': 40, 'num_layers': 4, 'dropout_rate': 0.27775193907239104, 'lr': 0.00015818278731402442}. Best is trial 1 with value: 2.209451403238077.\n",
      "[I 2024-10-31 14:54:34,216] Trial 3 finished with value: 2.146411110869551 and parameters: {'num_heads': 5, 'embed_dim': 160, 'num_layers': 3, 'dropout_rate': 0.3835173082401955, 'lr': 0.0004990513544197793}. Best is trial 3 with value: 2.146411110869551.\n",
      "[I 2024-10-31 14:56:24,692] Trial 4 finished with value: 2.7450587432996363 and parameters: {'num_heads': 7, 'embed_dim': 196, 'num_layers': 1, 'dropout_rate': 0.46375402828301604, 'lr': 4.1161255711351786e-05}. Best is trial 3 with value: 2.146411110869551.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Loss: 2.146411110869551\n",
      "  Best hyperparameters: \n",
      "    num_heads: 5\n",
      "    embed_dim: 160\n",
      "    num_layers: 3\n",
      "    dropout_rate: 0.3835173082401955\n",
      "    lr: 0.0004990513544197793\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Define the objective function for Optuna to optimize\n",
    "def objective(trial):\n",
    "    # Suggest values for the hyperparameters to tune\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8)\n",
    "    embed_dim = trial.suggest_int(\"embed_dim\", num_heads * 8, num_heads * 32, step=num_heads * 4)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Ensure embed_dim is divisible by num_heads\n",
    "    if embed_dim % num_heads != 0:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # Define the model with the suggested hyperparameters\n",
    "    model = Transformer(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        seq_len=seq_len,\n",
    "        output_size=output_size,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop parameters\n",
    "    epochs = 5\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for input_seq, target_seq in train_dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            outputs = model(input_seq)\n",
    "            \n",
    "            # Apply the appropriate task mode setup for loss calculation\n",
    "            if task_mode == 'text_gen':\n",
    "                target_seq = target_seq.contiguous().view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode in ['sort', 'seq_rev']:\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            if task_mode == 'copy':\n",
    "                target_seq = target_seq.view(-1)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "            \n",
    "            if task_mode == 'basic_arith':\n",
    "                outputs = outputs[:, :target_seq.size(1), :]\n",
    "                outputs = outputs.reshape(-1, vocab_size)\n",
    "                target_seq = target_seq.reshape(-1)\n",
    "                loss = criterion(outputs, target_seq)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Return the average loss as the metric to minimize\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    return epoch_loss\n",
    "\n",
    "# Optuna study setup\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=5)  # Number of trials to search for optimal hyperparameters\n",
    "\n",
    "# Print the best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Loss: {trial.value}\")\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
